{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing the datasets\n",
    "\n",
    "\n",
    "#import scikit-learn and dataset\n",
    "\n",
    "# Part a and b\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = [ 'comp.graphics','comp.os.ms-windows.misc','comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware','rec.autos','rec.motorcycles','rec.sport.baseball','rec.sport.hockey']\n",
    "twenty_all = fetch_20newsgroups(subset = 'all', categories = categories, shuffle=True, random_state=42)\n",
    "twenty_train = fetch_20newsgroups(subset = 'train', categories = categories, shuffle=True, random_state=42)\n",
    "twenty_test = fetch_20newsgroups(subset = 'test', categories = categories, shuffle=True, random_state=42)\n",
    "length_train = (len(twenty_train.data))\n",
    "length_test = (len(twenty_test.data))\n",
    "length_all = len(twenty_all.data)\n",
    "\n",
    "# Part c\n",
    "\n",
    "twenty_every = fetch_20newsgroups(subset = 'all',shuffle=True, random_state=42)\n",
    "length_every = (len(twenty_every.data))\n",
    "\n",
    "# Multi-class classification\n",
    "categories_multi = ['comp.sys.ibm.pc.hardware','comp.sys.mac.hardware','misc.forsale','soc.religion.christian']\n",
    "twenty_multi_train = fetch_20newsgroups(subset = 'train', categories = categories_multi, shuffle=True, random_state=42)\n",
    "twenty_multi_test = fetch_20newsgroups(subset = 'test', categories = categories_multi, shuffle=True, random_state=42)\n",
    "length_multi_train = (len(twenty_multi_train.data))\n",
    "length_multi_test = (len(twenty_multi_test.data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Draw the histogram.\n",
    "\n",
    "\n",
    "categories_hist = [ 'comp.graphics','comp.os.ms-windows.misc','comp.sys.ibm.pc.hardware','comp.sys.mac.hardware','rec.autos','rec.motorcycles',\n",
    "'rec.sport.baseball',\n",
    "'rec.sport.hockey']\n",
    "\n",
    "categories_short = [ 'graphics','ms-windows.misc','ibm.pc.hardware','mac.hardware','autos','motorcycles',\n",
    "'baseball',\n",
    "'hockey']\n",
    "\n",
    "\n",
    "twenty_hist = fetch_20newsgroups(subset='all', categories=categories_hist, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "freq0 = 0;\n",
    "freq1 = 0;\n",
    "freq2 = 0;\n",
    "freq3 = 0;\n",
    "freq4 = 0;\n",
    "freq5 = 0;\n",
    "freq6 = 0;\n",
    "freq7 = 0;\n",
    "freq8 = 0;\n",
    "\n",
    "for i in range (0,7882):\n",
    "    if twenty_hist.target[i] == 0:\n",
    "        freq0 +=1\n",
    "    elif twenty_hist.target[i]== 1:\n",
    "        freq1 +=1\n",
    "    elif twenty_hist.target[i] == 2:\n",
    "        freq2 +=1\n",
    "    elif twenty_hist.target[i]== 3:\n",
    "        freq3 +=1\n",
    "    elif twenty_hist.target[i]== 4:\n",
    "        freq4 +=1\n",
    "    elif twenty_hist.target[i]== 5:\n",
    "        freq5 +=1\n",
    "    elif twenty_hist.target[i]== 6:\n",
    "        freq6 +=1\n",
    "    else:\n",
    "        freq7 +=1        \n",
    "\n",
    "      \n",
    "\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Learn about API authentication here: https://plot.ly/python/getting-started\n",
    "# Find your api_key here: https://plot.ly/settings/api\n",
    "\n",
    "plt.bar([0,1,2,3,4,5,6,7], [freq0,freq1,freq2,freq3,freq4,freq5,freq6,freq7],  color=\"blue\",align = \"center\")\n",
    "plt.xticks([0,1,2,3,4,5,6,7], categories_short)\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#Part a and b\n",
    "\n",
    "import nltk, re, pprint\n",
    "\n",
    "\n",
    "#Preprocessing the train data\n",
    "\n",
    "twenty_train_new = list(twenty_train.data) # Modified dataset\n",
    "porter_train = nltk.LancasterStemmer() # Creating the instance of stemming to be used\n",
    "\n",
    "for i in range(0,length_train):\n",
    "  temp_train = twenty_train.data[i].split() # Tokenizing the document\n",
    "  temp_2_train = [porter_train.stem(t) for t in temp_train] # Replacing the stemming with root words\n",
    "  twenty_train_new[i] = ' '.join(temp_2_train)\n",
    "\n",
    "\n",
    "#Preprocessing the test data\n",
    "\n",
    "twenty_test_new = list(twenty_test.data) # Modified dataset\n",
    "porter_test = nltk.LancasterStemmer() # Creating the instance of stemming to be used\n",
    "\n",
    "for k in range(0,length_test):\n",
    "  temp_test = twenty_test.data[k].split() # Tokenizing the document\n",
    "  temp_2_test = [porter_test.stem(t) for t in temp_test] # Replacing the stemming with root words\n",
    "  twenty_test_new[k] = ' '.join(temp_2_test)\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Part c\n",
    "\n",
    "twenty_every_new = list(twenty_every.data) # Modified dataset\n",
    "porter_every = nltk.LancasterStemmer() # Creating the instance of stemming to be used\n",
    "\n",
    "for j in range(0,length_every):\n",
    "  temp_every = twenty_every.data[i].split() # Tokenizing the document\n",
    "  temp_2_every = [porter_every.stem(t) for t in temp_every] # Replacing the stemming with root words\n",
    "  twenty_every_new[i] = ' '.join(temp_2_every)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Multiclass classification\n",
    "\n",
    "\n",
    "#Preprocessing the train data\n",
    "\n",
    "twenty_multi_train_new = list(twenty_multi_train.data) # Modified dataset\n",
    "porter_multi_train = nltk.LancasterStemmer() # Creating the instance of stemming to be used\n",
    "\n",
    "for a in range(0,length_multi_train):\n",
    "  temp_multi_train = twenty_multi_train.data[a].split() # Tokenizing the document\n",
    "  temp_2_multi_train = [porter_multi_train.stem(t) for t in temp_multi_train] # Replacing the stemming with root words\n",
    "  twenty_multi_train_new[a] = ' '.join(temp_2_multi_train)\n",
    "\n",
    "\n",
    "#Preprocessing the test data\n",
    "\n",
    "twenty_multi_test_new = list(twenty_multi_test.data) # Modified dataset\n",
    "porter_multi_test = nltk.LancasterStemmer() # Creating the instance of stemming to be used\n",
    "\n",
    "for b in range(0,length_multi_test):\n",
    "  temp_multi_test = twenty_multi_test.data[b].split() # Tokenizing the document\n",
    "  temp_2_multi_test = [porter_multi_test.stem(t) for t in temp_multi_test] # Replacing the stemming with root words\n",
    "  twenty_multi_test_new[b] = ' '.join(temp_2_multi_test)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4732, 76726)\n",
      "(3150, 76726)\n",
      "(18846, 173462)\n",
      "(2352, 29385)\n",
      "(1565, 29385)\n"
     ]
    }
   ],
   "source": [
    "# Creating the term document matrix with the modified dataset\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "# Part a and b\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "stopWords = text.ENGLISH_STOP_WORDS # Creating the array of stop words\n",
    "\n",
    "# Creating the term-document matrix for training dataset\n",
    "\n",
    "count_vect_train = CountVectorizer(stop_words = stopWords) # Creating an object of class CountVectorizer.\n",
    "t_doc_train = count_vect_train.fit_transform(twenty_train_new) # Creating the term document matrix\n",
    "print(t_doc_train.shape)\n",
    "\n",
    "# Creating the term-document matrix for test dataset\n",
    "\n",
    "\n",
    "t_doc_test = count_vect_train.transform(twenty_test_new) # Creating the term document matrix\n",
    "print(t_doc_test.shape)\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "#Part c\n",
    "\n",
    "count_vect_every = CountVectorizer(stop_words = stopWords) # Creating an object of class CountVectorizer.\n",
    "t_doc_every = count_vect_every.fit_transform(twenty_every_new) # Creating the term document matrix\n",
    "print(t_doc_every.shape)\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Multiclass classification\n",
    "\n",
    "# Creating the term-document matrix for training dataset\n",
    "\n",
    "count_vect_multi_train = CountVectorizer(stop_words = stopWords) # Creating an object of class CountVectorizer.\n",
    "t_doc_multi_train = count_vect_multi_train.fit_transform(twenty_multi_train_new) # Creating the term document matrix\n",
    "print(t_doc_multi_train.shape)\n",
    "\n",
    "# Creating the term-document matrix for test dataset\n",
    "\n",
    "t_doc_multi_test = count_vect_multi_train.transform(twenty_multi_test_new) # Creating the term document matrix\n",
    "print(t_doc_multi_test.shape)\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Part c. This block of code is under construction and will be completed soon\n",
    "\n",
    "# Processing to convert the above term-document matrix to term-class matrix\n",
    "\n",
    "# getting the class info\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "class_every = twenty_every.target\n",
    "#print(twenty_every.target_names[7])\n",
    "# initializing arrays that holds class indices\n",
    "\n",
    "class_1 = []\n",
    "class_2 = []\n",
    "class_3 = []\n",
    "class_4 = []\n",
    "class_5 = []\n",
    "class_6 = []\n",
    "class_7 = []\n",
    "class_8 = []\n",
    "class_9 = []\n",
    "class_10 = []\n",
    "class_11 = []\n",
    "class_12 = []\n",
    "class_13 = []\n",
    "class_14 = []\n",
    "class_15 = []\n",
    "class_16 = []\n",
    "class_17 = []\n",
    "class_18 = []\n",
    "class_19 = []\n",
    "class_20 = []\n",
    "\n",
    "\n",
    "for m in range(0,length_every):\n",
    "    if class_every[m] == 0:\n",
    "        class_1.append(m)\n",
    "    \n",
    "    elif class_every[m] == 1:\n",
    "        class_2.append(m)\n",
    "    \n",
    "    elif class_every[m] == 2:\n",
    "        class_3.append(m)\n",
    "    \n",
    "    elif class_every[m] == 3:\n",
    "        class_4.append(m)\n",
    "    \n",
    "    elif class_every[m] == 4:\n",
    "        class_5.append(m)\n",
    "    \n",
    "    elif class_every[m] == 5:\n",
    "        class_6.append(m)\n",
    "    \n",
    "    elif class_every[m] == 6:\n",
    "        class_7.append(m)\n",
    "    \n",
    "    elif class_every[m] == 7:\n",
    "        class_8.append(m)\n",
    "    \n",
    "    elif class_every[m] == 8:\n",
    "        class_9.append(m)\n",
    "    \n",
    "    elif class_every[m] == 9:\n",
    "        class_10.append(m)\n",
    "    \n",
    "    elif class_every[m] == 10:\n",
    "        class_11.append(m)\n",
    "    \n",
    "    elif class_every[m] == 11:\n",
    "        class_12.append(m)\n",
    "    \n",
    "    elif class_every[m] == 12:\n",
    "        class_13.append(m)\n",
    "    \n",
    "    elif class_every[m] == 13:\n",
    "        class_14.append(m)\n",
    "    \n",
    "    elif class_every[m] == 14:\n",
    "        class_15.append(m)\n",
    "    \n",
    "    elif class_every[m] == 15:\n",
    "        class_16.append(m)\n",
    "    \n",
    "    elif class_every[m] == 16:\n",
    "        class_17.append(m)\n",
    "    \n",
    "    elif class_every[m] == 17:\n",
    "        class_18.append(m)\n",
    "    \n",
    "    elif class_every[m] == 19:\n",
    "        class_19.append(m)\n",
    "    \n",
    "    else:\n",
    "        class_20.append(m)\n",
    "        \n",
    "#class_list = [class_1,class_2,class_3,class_4,class_5,class_6,class_7,class_8,class_9,class_10,class_11,class_12,class_13,class_14,\n",
    "      #       class_15,class_16,class_17,class_18,class_19]\n",
    "        \n",
    "# Create the Term-Class matrix:\n",
    "\n",
    "sum_1= t_doc_every[class_1[0],:];\n",
    "for j in range(1,len(class_1)):\n",
    "    sum_1 = sum_1 + t_doc_every[class_1[j],:]\n",
    "\n",
    "sum_2= t_doc_every[class_2[0],:];\n",
    "for j in range(1,len(class_2)):\n",
    "    sum_2 = sum_2 + t_doc_every[class_2[j],:]\n",
    "    \n",
    "sum_3= t_doc_every[class_3[0],:];\n",
    "for j in range(1,len(class_3)):\n",
    "    sum_3 = sum_3 + t_doc_every[class_3[j],:]\n",
    "\n",
    "sum_4= t_doc_every[class_4[0],:];\n",
    "for j in range(1,len(class_4)):\n",
    "    sum_4 = sum_4 + t_doc_every[class_4[j],:]\n",
    "    \n",
    "sum_5= t_doc_every[class_5[0],:];\n",
    "for j in range(1,len(class_5)):\n",
    "    sum_5 = sum_5 + t_doc_every[class_5[j],:]\n",
    "    \n",
    "sum_6= t_doc_every[class_6[0],:];\n",
    "for j in range(1,len(class_6)):\n",
    "    sum_6 = sum_6 + t_doc_every[class_6[j],:]\n",
    "    \n",
    "sum_7= t_doc_every[class_7[0],:];\n",
    "for j in range(1,len(class_7)):\n",
    "    sum_7 = sum_7 + t_doc_every[class_7[j],:]\n",
    "\n",
    "sum_8= t_doc_every[class_8[0],:];\n",
    "for j in range(1,len(class_8)):\n",
    "    sum_8 = sum_8 + t_doc_every[class_8[j],:]\n",
    "\n",
    "sum_9= t_doc_every[class_9[0],:];\n",
    "for j in range(1,len(class_9)):\n",
    "    sum_9 = sum_9 + t_doc_every[class_9[j],:]\n",
    "\n",
    "sum_10= t_doc_every[class_10[0],:];\n",
    "for j in range(1,len(class_10)):\n",
    "    sum_10 = sum_10 + t_doc_every[class_10[j],:]\n",
    "\n",
    "sum_11= t_doc_every[class_11[0],:];\n",
    "for j in range(1,len(class_11)):\n",
    "    sum_11 = sum_11 + t_doc_every[class_11[j],:]\n",
    "\n",
    "sum_12= t_doc_every[class_12[0],:];\n",
    "for j in range(1,len(class_12)):\n",
    "    sum_12 = sum_12 + t_doc_every[class_12[j],:]\n",
    "    \n",
    "sum_13= t_doc_every[class_13[0],:];\n",
    "for j in range(1,len(class_13)):\n",
    "    sum_13 = sum_13 + t_doc_every[class_13[j],:]\n",
    "\n",
    "sum_14= t_doc_every[class_14[0],:];\n",
    "for j in range(1,len(class_14)):\n",
    "    sum_14 = sum_14 + t_doc_every[class_14[j],:]\n",
    "    \n",
    "sum_15= t_doc_every[class_15[0],:];\n",
    "for j in range(1,len(class_15)):\n",
    "    sum_15 = sum_15 + t_doc_every[class_15[j],:]\n",
    "    \n",
    "sum_16= t_doc_every[class_16[0],:];\n",
    "for j in range(1,len(class_16)):\n",
    "    sum_16 = sum_16 + t_doc_every[class_16[j],:]\n",
    "    \n",
    "sum_17= t_doc_every[class_17[0],:];\n",
    "for j in range(1,len(class_17)):\n",
    "    sum_17 = sum_17 + t_doc_every[class_17[j],:]\n",
    "\n",
    "sum_18= t_doc_every[class_18[0],:];\n",
    "for j in range(1,len(class_18)):\n",
    "    sum_18 = sum_18 + t_doc_every[class_18[j],:]\n",
    "\n",
    "sum_19= t_doc_every[class_19[0],:];\n",
    "for j in range(1,len(class_19)):\n",
    "    sum_19 = sum_19 + t_doc_every[class_19[j],:]\n",
    "\n",
    "sum_20= t_doc_every[class_20[0],:];\n",
    "for j in range(1,len(class_20)):\n",
    "    sum_20 = sum_20 + t_doc_every[class_20[j],:]\n",
    "\n",
    "    \n",
    "t_class_every = vstack((sum_1,sum_2,sum_3,sum_4,sum_5,sum_6,sum_7,sum_8,sum_9,sum_10,sum_11,sum_12,sum_13,sum_14,sum_15,\n",
    "                            sum_16,sum_17,sum_18,sum_19,sum_20));\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Words for PC Hardware (in reverse order):\n",
      "card\n",
      "controller\n",
      "organization\n",
      "subject\n",
      "lines\n",
      "drive\n",
      "ide\n",
      "com\n",
      "edu\n",
      "scsi\n",
      "\n",
      "\n",
      "Common Words for MAC Hardware (in reverse order):\n",
      "university\n",
      "scsi\n",
      "centris\n",
      "quadra\n",
      "apple\n",
      "organization\n",
      "mac\n",
      "subject\n",
      "lines\n",
      "edu\n",
      "\n",
      "\n",
      "Common Words for 'For Sale' (in reverse order):\n",
      "10\n",
      "new\n",
      "com\n",
      "university\n",
      "organization\n",
      "sale\n",
      "lines\n",
      "subject\n",
      "00\n",
      "edu\n",
      "\n",
      "\n",
      "Common Words for Religion.Christian (in reverse order):\n",
      "athos\n",
      "lines\n",
      "christ\n",
      "people\n",
      "church\n",
      "subject\n",
      "jesus\n",
      "christians\n",
      "edu\n",
      "god\n"
     ]
    }
   ],
   "source": [
    "#part c continued\n",
    "\n",
    "#create TF-ICF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "\n",
    "tficf_transformer = TfidfTransformer() # Creating an object of class TfidfTransformer\n",
    "\n",
    "\n",
    "tficf = tficf_transformer.fit_transform(t_class_every) # Creating the tfidf matrix\n",
    "\n",
    "    \n",
    "sorted_pc_hardware = np.argsort(tficf[3,:].toarray()) \n",
    "sorted_mac_hardware = np.argsort(tficf[4,:].toarray())\n",
    "sorted_forsale= np.argsort(tficf[6,:].toarray())\n",
    "sorted_rel_christian = np.argsort(tficf[15,:].toarray())\n",
    "\n",
    "words = np.asarray(count_vect_every.get_feature_names())\n",
    "\n",
    "#print(words[170325])\n",
    "print(\"Common Words for PC Hardware (in reverse order):\")\n",
    "for i in range(len(words)-10,len(words)):\n",
    "    print(words[sorted_pc_hardware[0,i]])\n",
    "\n",
    "print('\\n')   \n",
    "print(\"Common Words for MAC Hardware (in reverse order):\")\n",
    "    \n",
    "for i in range(len(words)-10,len(words)):\n",
    "    print(words[sorted_mac_hardware[0,i]])\n",
    "\n",
    "print('\\n')   \n",
    "print(\"Common Words for 'For Sale' (in reverse order):\")    \n",
    "for i in range(len(words)-10,len(words)):\n",
    "    print(words[sorted_forsale[0,i]])\n",
    "    \n",
    "    \n",
    "print('\\n')   \n",
    "print(\"Common Words for Religion.Christian (in reverse order):\")\n",
    "for i in range(len(words)-10,len(words)):\n",
    "    print(words[sorted_rel_christian[0,i]])\n",
    "    \n",
    "    \n",
    "\n",
    "#3,4,6,15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4732, 76726)\n",
      "(3150, 76726)\n",
      "(2352, 29385)\n",
      "(1565, 29385)\n"
     ]
    }
   ],
   "source": [
    "# Creating the tfidf matrix with the modified dataset\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "# Binary classification\n",
    "\n",
    "\n",
    "# Creating the tf-idf for training dataset\n",
    "\n",
    "tfidf_transformer_train = TfidfTransformer() # Creating an object of class TfidfTransformer\n",
    "tfidf_train = tfidf_transformer_train.fit_transform(t_doc_train) # Creating the tfidf matrix\n",
    "print(tfidf_train.shape)\n",
    "\n",
    "# Creating the tf-idf for test dataset\n",
    "\n",
    "\n",
    "tfidf_test = tfidf_transformer_train.transform(t_doc_test) # Creating the tfidf matrix\n",
    "print(tfidf_test.shape)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "#Multiclass classification\n",
    "\n",
    "# Creating the tf-idf for training dataset\n",
    "\n",
    "tfidf_transformer_multi_train = TfidfTransformer() # Creating an object of class TfidfTransformer\n",
    "tfidf_multi_train = tfidf_transformer_multi_train.fit_transform(t_doc_multi_train) # Creating the tfidf matrix\n",
    "print(tfidf_multi_train.shape)\n",
    "\n",
    "\n",
    "# Creating the tf-idf for test dataset\n",
    "\n",
    "tfidf_multi_test = tfidf_transformer_multi_train.transform(t_doc_multi_test) # Creating the tfidf matrix\n",
    "print(tfidf_multi_test.shape)\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4732, 50)\n",
      "(3150, 50)\n",
      "(2352, 50)\n",
      "(1565, 50)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Applying LSI to training and test tf-idf matrix\n",
    "\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "# Binary classification\n",
    "\n",
    "\n",
    "# Applying to the training set\n",
    "\n",
    "lsa_transformer_train = TruncatedSVD(50, random_state = 42 )\n",
    "lsa_train = lsa_transformer_train.fit_transform(tfidf_train)\n",
    "print(lsa_train.shape)\n",
    "\n",
    "# Applying to the test set\n",
    "\n",
    "\n",
    "lsa_test = lsa_transformer_train.transform(tfidf_test)\n",
    "print(lsa_test.shape)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "# Multiclass classification\n",
    "\n",
    "# Applying to the training set\n",
    "\n",
    "lsa_transformer_multi_train = TruncatedSVD(50, random_state = 42 )\n",
    "lsa_multi_train = lsa_transformer_multi_train.fit_transform(tfidf_multi_train)\n",
    "print(lsa_multi_train.shape)\n",
    "\n",
    "\n",
    "# Applying to the test set\n",
    "\n",
    "\n",
    "lsa_multi_test = lsa_transformer_multi_train.transform(tfidf_multi_test)\n",
    "print(lsa_multi_test.shape)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# Note: This lsa_train matrix will be used in training the classifiers and lsa_test matrix will be used to test the performance of the classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Processing for classification\n",
    "\n",
    "# Creating the array of class labels for binary classification\n",
    "\n",
    "# Training dataset\n",
    "\n",
    "class_train = []\n",
    "\n",
    "for a in range(0,length_train):\n",
    "    if twenty_train.target[a] < 4:\n",
    "        class_train.append(0)\n",
    "    \n",
    "    else:\n",
    "        class_train.append(1)\n",
    "\n",
    "\n",
    "#print(class_train[0])\n",
    "\n",
    "# Test dataset\n",
    "\n",
    "class_test = []\n",
    "\n",
    "for b in range(0,length_test):\n",
    "    if twenty_test.target[b] < 4:\n",
    "        class_test.append(0)\n",
    "    \n",
    "    else:\n",
    "        class_test.append(1)\n",
    "\n",
    "#print(len(class_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM classifier results\n",
      "\n",
      "Accuracy score\n",
      "0.966984126984\n",
      "\n",
      "Classification report\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "  Computer Technology       0.98      0.95      0.97      1560\n",
      "Recreational Activity       0.96      0.98      0.97      1590\n",
      "\n",
      "          avg / total       0.97      0.97      0.97      3150\n",
      "\n",
      "\n",
      "Confusion matrix\n",
      "[[1488   72]\n",
      " [  32 1558]]\n",
      "\n",
      "--------------------------------------------------------------------------------------\n",
      "Soft margin SVM classifier results for the optimal C with 5-fold CV\n",
      "\n",
      "Optimal C\n",
      "\n",
      "{'C': 10}\n",
      "\n",
      "Accuracy score\n",
      "0.967936507937\n",
      "\n",
      "Classification report\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "  Computer Technology       0.98      0.96      0.97      1560\n",
      "Recreational Activity       0.96      0.98      0.97      1590\n",
      "\n",
      "          avg / total       0.97      0.97      0.97      3150\n",
      "\n",
      "\n",
      "Confusion matrix\n",
      "[[1496   64]\n",
      " [  37 1553]]\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Gaussian NB classifier results\n",
      "\n",
      "Accuracy score\n",
      "0.818095238095\n",
      "\n",
      "Classification report\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "  Computer Technology       0.75      0.94      0.84      1560\n",
      "Recreational Activity       0.92      0.70      0.79      1590\n",
      "\n",
      "          avg / total       0.84      0.82      0.82      3150\n",
      "\n",
      "\n",
      "Confusion matrix\n",
      "[[1469   91]\n",
      " [ 482 1108]]\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "Logistic regression classifier results\n",
      "\n",
      "Accuracy score\n",
      "0.960952380952\n",
      "\n",
      "Classification report\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "  Computer Technology       0.97      0.95      0.96      1560\n",
      "Recreational Activity       0.95      0.97      0.96      1590\n",
      "\n",
      "          avg / total       0.96      0.96      0.96      3150\n",
      "\n",
      "\n",
      "Confusion matrix\n",
      "[[1479   81]\n",
      " [  42 1548]]\n",
      "\n",
      "------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Binary Classification\n",
    "\n",
    "# Importing the required classifiers\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "# Linear SVM classifier (Hard Margin)\n",
    "\n",
    "clf_lin_svm = LinearSVC(random_state=42).fit(lsa_train,class_train)\n",
    "predicted_lin_svm = clf_lin_svm.predict(lsa_test)\n",
    "lin_svm_acc = accuracy_score(class_test, predicted_lin_svm)\n",
    "print('Linear SVM classifier results')\n",
    "print()\n",
    "print('Accuracy score')\n",
    "print(lin_svm_acc)\n",
    "print()\n",
    "print('Classification report')\n",
    "print(metrics.classification_report(class_test, predicted_lin_svm,\n",
    "    target_names=['Computer Technology', 'Recreational Activity']))\n",
    "\n",
    "print()\n",
    "lin_svm_metric = metrics.confusion_matrix(class_test, predicted_lin_svm)\n",
    "print('Confusion matrix')\n",
    "print(lin_svm_metric)\n",
    "print()\n",
    "print('--------------------------------------------------------------------------------------')\n",
    "\n",
    "# ROC plotting for Linear SVM classifier (Hard Margin)\n",
    "\n",
    "fpr_svm = []\n",
    "tpr_svm = []\n",
    "roc_auc_svm = []\n",
    "scores_svm = clf_lin_svm.decision_function(lsa_test)\n",
    "\n",
    "fpr_svm, tpr_svm, thresholds_svm = metrics.roc_curve(class_test,scores_svm)\n",
    "roc_auc_svm = auc(fpr_svm,tpr_svm)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr_svm, tpr_svm, label='ROC curve (area = %0.4f)' % roc_auc_svm)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating for Linear SVM')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# Linear SVM classifier (Soft Margin)\n",
    "\n",
    "clf_svm_soft = GridSearchCV(LinearSVC(), param_grid={'C': [0.001,0.01,0.1,1,10,100,1000]},cv = 5, scoring = 'accuracy')\n",
    "clf_svm_soft.fit(lsa_train,class_train)\n",
    "print('Soft margin SVM classifier results for the optimal C with 5-fold CV')\n",
    "print()\n",
    "print('Optimal C')\n",
    "print()\n",
    "print(clf_svm_soft.best_params_)\n",
    "print()\n",
    "predicted_svm_soft = clf_svm_soft.predict(lsa_test)\n",
    "svm_soft_acc = accuracy_score(class_test, predicted_svm_soft)\n",
    "print('Accuracy score')\n",
    "print(svm_soft_acc)\n",
    "print()\n",
    "print('Classification report')\n",
    "print(metrics.classification_report(class_test, predicted_svm_soft,\n",
    "    target_names=['Computer Technology', 'Recreational Activity']))\n",
    "\n",
    "print()\n",
    "print('Confusion matrix')\n",
    "svm_soft_metric = metrics.confusion_matrix(class_test, predicted_svm_soft)\n",
    "print(svm_soft_metric)\n",
    "print()\n",
    "print('---------------------------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Gaussian NB classifier\n",
    "\n",
    "clf_gauss_nb = GaussianNB().fit(lsa_train,class_train)\n",
    "predicted_gauss_nb = clf_gauss_nb.predict(lsa_test)\n",
    "gaussian_nb_acc = accuracy_score(class_test, predicted_gauss_nb)\n",
    "print('Gaussian NB classifier results')\n",
    "print()\n",
    "print('Accuracy score')\n",
    "print(gaussian_nb_acc)\n",
    "print()\n",
    "print('Classification report')\n",
    "print(metrics.classification_report(class_test, predicted_gauss_nb,\n",
    "    target_names=['Computer Technology', 'Recreational Activity']))\n",
    "print()\n",
    "gaussian_nb_metric = metrics.confusion_matrix(class_test, predicted_gauss_nb)\n",
    "print('Confusion matrix')\n",
    "print(gaussian_nb_metric)\n",
    "print()\n",
    "print('--------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ROC plotting for gaussian NB\n",
    "\n",
    "fpr_nb = []\n",
    "tpr_nb = []\n",
    "roc_auc_nb = []\n",
    "scores_nb = clf_gauss_nb.predict_proba(lsa_test)\n",
    "fpr_nb, tpr_nb, thresholds_nb = metrics.roc_curve(class_test,scores_nb[:,1])\n",
    "roc_auc_nb = auc(fpr_nb,tpr_nb)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr_nb, tpr_nb, label='ROC curve (area = %0.4f)' % roc_auc_nb)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for Gaussian Naive Bayes')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Logistic regression classifier\n",
    "\n",
    "clf_logistic = LogisticRegression().fit(lsa_train,class_train)\n",
    "predicted_logistic = clf_logistic.predict(lsa_test)\n",
    "print('Logistic regression classifier results')\n",
    "print()\n",
    "logistic_acc = accuracy_score(class_test, predicted_logistic)\n",
    "print('Accuracy score')\n",
    "print(logistic_acc)\n",
    "print()\n",
    "print('Classification report')\n",
    "print(metrics.classification_report(class_test, predicted_logistic,\n",
    "    target_names=['Computer Technology', 'Recreational Activity']))\n",
    "print()\n",
    "logistic_metric = metrics.confusion_matrix(class_test, predicted_logistic)\n",
    "print('Confusion matrix')\n",
    "print(logistic_metric)\n",
    "print()\n",
    "print('------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ROC plotting for logistic\n",
    "\n",
    "fpr_logistic = []\n",
    "tpr_logistic = []\n",
    "roc_auc_logistic = []\n",
    "scores_logistic = clf_logistic.decision_function(lsa_test)\n",
    "\n",
    "fpr_logistic, tpr_logistic, thresholds_logistic = metrics.roc_curve(class_test,scores_logistic)\n",
    "roc_auc_logistic = auc(fpr_logistic,tpr_logistic)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr_logistic, tpr_logistic, label='ROC curve (area = %0.4f)' % roc_auc_logistic)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating for Logistic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plotting the ROC curves on the same plot for comparison\n",
    "\n",
    "plt.plot(fpr_svm, tpr_svm,'r', label='ROC curve (area = %0.4f)' % roc_auc_svm)\n",
    "plt.plot(fpr_nb, tpr_nb,'b', label='ROC curve (area = %0.4f)' % roc_auc_nb)\n",
    "plt.plot(fpr_logistic, tpr_logistic,'g', label='ROC curve (area = %0.4f)' % roc_auc_logistic)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Comparison')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM OVO classifier results\n",
      "\n",
      "Accuracy score\n",
      "0.85750798722\n",
      "\n",
      "Classification report\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "comp.sys.ibm.pc.hardware       0.77      0.81      0.79       392\n",
      "   comp.sys.mac.hardware       0.86      0.78      0.82       385\n",
      "            misc.forsale       0.82      0.90      0.86       390\n",
      "  soc.religion.christian       0.99      0.94      0.97       398\n",
      "\n",
      "             avg / total       0.86      0.86      0.86      1565\n",
      "\n",
      "\n",
      "Confusion matrix\n",
      "[[317  38  37   0]\n",
      " [ 55 300  29   1]\n",
      " [ 28  11 350   1]\n",
      " [ 11   1  11 375]]\n",
      "\n",
      "--------------------------------------------------------------------------------------\n",
      "SVM OVR classifier results\n",
      "\n",
      "Accuracy score\n",
      "0.875399361022\n",
      "\n",
      "Classification report\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "comp.sys.ibm.pc.hardware       0.83      0.79      0.81       392\n",
      "   comp.sys.mac.hardware       0.84      0.82      0.83       385\n",
      "            misc.forsale       0.84      0.92      0.88       390\n",
      "  soc.religion.christian       0.99      0.97      0.98       398\n",
      "\n",
      "             avg / total       0.88      0.88      0.88      1565\n",
      "\n",
      "\n",
      "Confusion matrix\n",
      "[[309  47  35   1]\n",
      " [ 39 315  30   1]\n",
      " [ 20  10 358   2]\n",
      " [  4   1   5 388]]\n",
      "\n",
      "--------------------------------------------------------------------------------------\n",
      "Gaussian NB classifier results\n",
      "\n",
      "Accuracy score\n",
      "0.63642172524\n",
      "\n",
      "Classification report\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "comp.sys.ibm.pc.hardware       0.65      0.55      0.60       392\n",
      "   comp.sys.mac.hardware       0.65      0.32      0.43       385\n",
      "            misc.forsale       0.45      0.82      0.58       390\n",
      "  soc.religion.christian       0.99      0.85      0.91       398\n",
      "\n",
      "             avg / total       0.69      0.64      0.63      1565\n",
      "\n",
      "\n",
      "Confusion matrix\n",
      "[[217  30 144   1]\n",
      " [ 77 124 182   2]\n",
      " [ 36  35 318   1]\n",
      " [  2   1  58 337]]\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Multiclass classification\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "# SVM OVO \n",
    "\n",
    "clf_svm_ovo = svm.SVC(decision_function_shape='ovo',kernel = 'linear',random_state=42)\n",
    "clf_svm_ovo.fit(lsa_multi_train,twenty_multi_train.target)\n",
    "predicted_svm_ovo = clf_svm_ovo.predict(lsa_multi_test)\n",
    "svm_ovo_acc = accuracy_score(twenty_multi_test.target, predicted_svm_ovo)\n",
    "print('SVM OVO classifier results')\n",
    "print()\n",
    "print('Accuracy score')\n",
    "print(svm_ovo_acc)\n",
    "print()\n",
    "print('Classification report')\n",
    "print(metrics.classification_report(twenty_multi_test.target, predicted_svm_ovo,\n",
    "    target_names=['comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware','misc.forsale','soc.religion.christian']))\n",
    "\n",
    "print()\n",
    "svm_ovo_metric = metrics.confusion_matrix(twenty_multi_test.target, predicted_svm_ovo)\n",
    "print('Confusion matrix')\n",
    "print(svm_ovo_metric)\n",
    "print()\n",
    "print('--------------------------------------------------------------------------------------')\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "# SVM OVR\n",
    "\n",
    "clf_svm_ovr = svm.LinearSVC(random_state = 42)\n",
    "clf_svm_ovr.fit(lsa_multi_train,twenty_multi_train.target)\n",
    "predicted_svm_ovr = clf_svm_ovr.predict(lsa_multi_test)\n",
    "svm_ovr_acc = accuracy_score(twenty_multi_test.target, predicted_svm_ovr)\n",
    "print('SVM OVR classifier results')\n",
    "print()\n",
    "print('Accuracy score')\n",
    "print(svm_ovr_acc)\n",
    "print()\n",
    "print('Classification report')\n",
    "print(metrics.classification_report(twenty_multi_test.target, predicted_svm_ovr,\n",
    "    target_names=['comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware','misc.forsale','soc.religion.christian']))\n",
    "\n",
    "print()\n",
    "svm_ovr_metric = metrics.confusion_matrix(twenty_multi_test.target, predicted_svm_ovr)\n",
    "print('Confusion matrix')\n",
    "print(svm_ovr_metric)\n",
    "print()\n",
    "print('--------------------------------------------------------------------------------------')\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "# Gaussian NB \n",
    "\n",
    "\n",
    "clf_gauss_nb_multi = GaussianNB().fit(lsa_multi_train,twenty_multi_train.target)\n",
    "predicted_gauss_nb_multi = clf_gauss_nb_multi.predict(lsa_multi_test)\n",
    "gaussian_nb_multi_acc = accuracy_score(twenty_multi_test.target, predicted_gauss_nb_multi)\n",
    "print('Gaussian NB classifier results')\n",
    "print()\n",
    "print('Accuracy score')\n",
    "print(gaussian_nb_multi_acc)\n",
    "print()\n",
    "print('Classification report')\n",
    "print(metrics.classification_report(twenty_multi_test.target, predicted_gauss_nb_multi,\n",
    "    target_names=['comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware','misc.forsale','soc.religion.christian']))\n",
    "print()\n",
    "gaussian_nb_multi_metric = metrics.confusion_matrix(twenty_multi_test.target, predicted_gauss_nb_multi)\n",
    "print('Confusion matrix')\n",
    "print(gaussian_nb_multi_metric)\n",
    "print()\n",
    "print('--------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
